{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "\n",
        "train_path = \"https://media.githubusercontent.com/media/PavelKilko/applied-application-of-neural-networks/refs/heads/master/lab-6-7/small-dataset/train.csv\"\n",
        "val_path = \"https://media.githubusercontent.com/media/PavelKilko/applied-application-of-neural-networks/refs/heads/master/lab-6-7/small-dataset/val.csv\"\n",
        "test_path = \"https://media.githubusercontent.com/media/PavelKilko/applied-application-of-neural-networks/refs/heads/master/lab-6-7/small-dataset/train.csv\"\n",
        "\n",
        "def preprocess_data(train_path, val_path, test_path):\n",
        "    train_dataset = pd.read_csv(train_path)\n",
        "    val_dataset = pd.read_csv(val_path)\n",
        "    test_dataset = pd.read_csv(test_path)\n",
        "\n",
        "    X_train = train_dataset.drop('font', axis=1).astype('float32').values\n",
        "    y_train = train_dataset['font'].values\n",
        "\n",
        "    X_val = val_dataset.drop('font', axis=1).astype('float32').values\n",
        "    y_val = val_dataset['font'].values\n",
        "\n",
        "    X_test = test_dataset.drop('font', axis=1).astype('float32').values\n",
        "    y_test = test_dataset['font'].values\n",
        "\n",
        "    X_train = X_train.reshape(-1, 20, 20, 1)\n",
        "    X_val = X_val.reshape(-1, 20, 20, 1)\n",
        "    X_test = X_test.reshape(-1, 20, 20, 1)\n",
        "\n",
        "    unique_labels = np.unique(y_train)\n",
        "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    y_train_encoded = np.array([label_to_index[label] for label in y_train])\n",
        "    y_val_encoded = np.array([label_to_index[label] for label in y_val])\n",
        "    y_test_encoded = np.array([label_to_index[label] for label in y_test])\n",
        "\n",
        "    num_classes = len(unique_labels)\n",
        "\n",
        "    y_train = to_categorical(y_train_encoded, num_classes=num_classes)\n",
        "    y_val = to_categorical(y_val_encoded, num_classes=num_classes)\n",
        "    y_test = to_categorical(y_test_encoded, num_classes=num_classes)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, num_classes, unique_labels\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, num_classes, unique_labels = preprocess_data(train_path, val_path, test_path)\n",
        "\n",
        "# Store the model globally so it can be saved on demand\n",
        "trained_model = None\n",
        "\n",
        "# Build and train model with user-defined parameters\n",
        "def build_and_train_model(filter1, filter2, filter3, kernel1, kernel2, kernel3, dense_neurons, learning_rate, dropout, patience_es, factor_rlr, patience_rlr, min_lr, epochs):\n",
        "    global trained_model\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Input(shape=(20, 20, 1)))\n",
        "\n",
        "    model.add(Conv2D(filter1, (kernel1, kernel1), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filter1, (kernel1, kernel1), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Conv2D(filter2, (kernel2, kernel2), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filter2, (kernel2, kernel2), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Conv2D(filter3, (kernel3, kernel3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filter3, (kernel3, kernel3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(dense_neurons, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience_es, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=factor_rlr, patience=patience_rlr, min_lr=min_lr)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=128,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    # Store the trained model globally\n",
        "    trained_model = model\n",
        "\n",
        "    # Plot training results\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Metrics')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Metrics')\n",
        "    plt.savefig('training_results.png')\n",
        "\n",
        "    return f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\", 'training_results.png'\n",
        "\n",
        "# Save model on demand\n",
        "def save_model():\n",
        "    global trained_model\n",
        "    if trained_model is None:\n",
        "        return None\n",
        "    model_path = \"trained_model.h5\"\n",
        "    trained_model.save(model_path)\n",
        "    return model_path\n",
        "\n",
        "# Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=build_and_train_model,\n",
        "    inputs=[\n",
        "        gr.Number(label=\"Filter Number (1st Layer)\", value=32),\n",
        "        gr.Number(label=\"Filter Number (2nd Layer)\", value=64),\n",
        "        gr.Number(label=\"Filter Number (3rd Layer)\", value=128),\n",
        "        gr.Number(label=\"Kernel Size (1st Layer)\", value=3),\n",
        "        gr.Number(label=\"Kernel Size (2nd Layer)\", value=3),\n",
        "        gr.Number(label=\"Kernel Size (3rd Layer)\", value=3),\n",
        "        gr.Number(label=\"Dense Layer Neurons\", value=256),\n",
        "        gr.Number(label=\"Learning Rate\", value=0.0005),\n",
        "        gr.Number(label=\"Dropout Value\", value=0.3),\n",
        "        gr.Number(label=\"Early Stopping Patience\", value=15),\n",
        "        gr.Number(label=\"Reduce LR Factor\", value=0.2),\n",
        "        gr.Number(label=\"Reduce LR Patience\", value=7),\n",
        "        gr.Number(label=\"Min LR\", value=1e-6),\n",
        "        gr.Number(label=\"Epochs\", value=100)\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Final Results\"),\n",
        "        gr.Image(label=\"Training Results Plot\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Separate interface for downloading the model\n",
        "download_interface = gr.Interface(\n",
        "    fn=save_model,\n",
        "    inputs=[],\n",
        "    outputs=gr.File(label=\"Download Trained Model\"),\n",
        "    live=False\n",
        ")\n",
        "\n",
        "# Combine both interfaces\n",
        "gr.TabbedInterface([interface, download_interface], [\"Train Model\", \"Download Model\"]).launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "4_qv-yK55wwZ",
        "outputId": "79ae0a2f-4096-4631-cd2f-39cf265d007b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://34e69349e42f360e08.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://34e69349e42f360e08.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 561ms/step - accuracy: 0.1713 - loss: 3.8778 - val_accuracy: 0.3576 - val_loss: 2.7165 - learning_rate: 5.0000e-04\n",
            "Epoch 1/3\n",
            "\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 565ms/step - accuracy: 0.1741 - loss: 3.8878 - val_accuracy: 0.3465 - val_loss: 2.6925 - learning_rate: 5.0000e-04\n",
            "Epoch 2/3\n",
            "\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 548ms/step - accuracy: 0.3282 - loss: 2.7981 - val_accuracy: 0.4020 - val_loss: 2.3693 - learning_rate: 5.0000e-04\n",
            "Epoch 3/3\n",
            "\u001b[1m244/244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 579ms/step - accuracy: 0.3897 - loss: 2.5030 - val_accuracy: 0.4350 - val_loss: 2.2204 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s14Img8exvU2",
        "outputId": "c752347e-a35f-4fe0-fcdc-bd43bcba2492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://eb75cb5e763fb1ada4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eb75cb5e763fb1ada4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7863 <> https://eb75cb5e763fb1ada4.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "import requests\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Constants for model and labels URLs\n",
        "MODEL_URL = \"https://github.com/PavelKilko/applied-application-of-neural-networks/raw/refs/heads/master/lab-9-10/small-font-recognizer-cnn-v1.1.h5\"  # Replace with the actual model URL\n",
        "LABELS_URL = \"https://raw.githubusercontent.com/PavelKilko/applied-application-of-neural-networks/refs/heads/master/lab-9-10/unique_labels.json\"  # Replace with the actual labels JSON URL\n",
        "\n",
        "\n",
        "# Helper function to download model and labels\n",
        "def download_model_and_labels(model_url, labels_url):\n",
        "    # Download the model\n",
        "    model_path = \"model.h5\"\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        f.write(requests.get(model_url).content)\n",
        "\n",
        "    # Download the labels\n",
        "    labels_path = \"labels.json\"\n",
        "    with open(labels_path, \"wb\") as f:\n",
        "        f.write(requests.get(labels_url).content)\n",
        "\n",
        "    # Load model and labels\n",
        "    try:\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "    except ValueError as e:\n",
        "        raise RuntimeError(\"Failed to load the model. Ensure the .h5 file is valid and matches the input shape.\") from e\n",
        "\n",
        "    with open(labels_path, \"r\") as f:\n",
        "        unique_labels = json.load(f)\n",
        "\n",
        "    return model, unique_labels\n",
        "\n",
        "\n",
        "# Helper function to process the image\n",
        "def preprocess_image(image):\n",
        "    # Convert the image to grayscale\n",
        "    grayscale_image = ImageOps.grayscale(image)\n",
        "\n",
        "    # Resize the image to the nearest multiple of 20 (if not already divisible)\n",
        "    width, height = grayscale_image.size\n",
        "    width -= width % 20\n",
        "    height -= height % 20\n",
        "    grayscale_image = grayscale_image.resize((width, height))\n",
        "\n",
        "    # Split into 20x20 squares\n",
        "    squares = []\n",
        "    for i in range(0, height, 20):\n",
        "        for j in range(0, width, 20):\n",
        "            square = grayscale_image.crop((j, i, j + 20, i + 20))\n",
        "            square_array = np.array(square).astype('float32') / 255.0\n",
        "            square_array = square_array.reshape(20, 20, 1)  # Add channel dimension\n",
        "            squares.append(square_array)\n",
        "\n",
        "    return np.array(squares), width // 20, height // 20, grayscale_image\n",
        "\n",
        "\n",
        "# Prediction function\n",
        "def predict(image):\n",
        "    # Download model and labels\n",
        "    model, unique_labels = download_model_and_labels(MODEL_URL, LABELS_URL)\n",
        "\n",
        "    # Preprocess the image\n",
        "    squares, grid_width, grid_height, grayscale_image = preprocess_image(image)\n",
        "\n",
        "    # Make predictions for each square\n",
        "    try:\n",
        "        predictions = model.predict(squares)\n",
        "    except ValueError as e:\n",
        "        raise RuntimeError(\"Failed to predict. Ensure the model input shape matches (20, 20, 1).\") from e\n",
        "\n",
        "    # Update to handle list-based unique_labels\n",
        "    predicted_labels = [unique_labels[np.argmax(pred)] for pred in predictions]\n",
        "\n",
        "    # Visualize results\n",
        "    # Visualize results\n",
        "    fig, ax = plt.subplots(grid_height, grid_width, figsize=(15, 15))\n",
        "\n",
        "    # Ensure ax is a 2D array\n",
        "    if grid_width == 1 and grid_height == 1:\n",
        "        ax = np.array([[ax]])  # Convert single Axes object to a 2D array\n",
        "    elif grid_width == 1 or grid_height == 1:\n",
        "        ax = np.expand_dims(ax, axis=0 if grid_height == 1 else 1)\n",
        "\n",
        "    for i in range(grid_height):\n",
        "        for j in range(grid_width):\n",
        "            square_idx = i * grid_width + j\n",
        "            ax[i, j].imshow(squares[square_idx].reshape(20, 20), cmap='gray')\n",
        "            ax[i, j].axis('off')\n",
        "            ax[i, j].set_title(predicted_labels[square_idx], fontsize=16)  # Increased font size\n",
        "\n",
        "    # Save the result visualization\n",
        "    result_path = \"predictions.png\"\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(result_path)\n",
        "    plt.close(fig)  # Ensure the figure is closed to avoid memory issues\n",
        "\n",
        "    return result_path\n",
        "\n",
        "\n",
        "# Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=[\n",
        "        gr.Image(label=\"Upload Image\", type=\"pil\")\n",
        "    ],\n",
        "    outputs=gr.Image(label=\"Predicted Squares with Labels\"),\n",
        "    live=False\n",
        ")\n",
        "\n",
        "interface.launch(share=True, debug=True)"
      ]
    }
  ]
}