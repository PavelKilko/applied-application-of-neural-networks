{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7da09c-2df9-4aa3-91a0-3c4d6ac6ca36",
   "metadata": {},
   "source": [
    "# Construction and training of a fully connected neural network for recognition of a large set of fonts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b1d4e-b236-4e46-a9e8-5e5535aa98c3",
   "metadata": {},
   "source": [
    "### DATASET PREPARING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb4b36-76ed-431f-a459-ef4c43fe9eef",
   "metadata": {},
   "source": [
    "##### Check available fonts in directory \"all-fonts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc8fec-1a6a-4f2c-b934-e2b974b7ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def list_files_in_folder(folder_path):\n",
    "    files_data = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        if (os.path.isfile(file_path)):\n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            files_data.append({\"filename\": file_name, \"size\": file_size})\n",
    "\n",
    "    df = pd.DataFrame(files_data)\n",
    "\n",
    "    df_sorted = df.sort_values(by=\"size\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991fc62-78e0-45ef-8575-c822e299ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list_files_in_folder(\"./all-fonts\")\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25079c67-f558-404c-a6e5-c6bbff96fe32",
   "metadata": {},
   "source": [
    "##### Creating font paths for all fonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc7bab-a1b3-47dc-b3ba-9ec86924cc39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "font_names = files[\"filename\"].tolist()\n",
    "font_paths = [\"./all-fonts/\" + font_name for font_name in font_names]\n",
    "font_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5a830-2767-4d42-9a4d-1117a1340488",
   "metadata": {},
   "source": [
    "##### Create dataset for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5cc0fb-2e2f-4b91-88bc-594c4de1ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def create_train_val_test_datasets(font_paths):\n",
    "    train_dataset = pd.DataFrame()\n",
    "    val_dataset = pd.DataFrame()\n",
    "    test_dataset = pd.DataFrame()\n",
    "\n",
    "    columns_to_drop = [\n",
    "        'fontVariant',\n",
    "        'm_label',\n",
    "        'strength',\n",
    "        'italic',\n",
    "        'orientation',\n",
    "        'm_top',\n",
    "        'm_left',\n",
    "        'originalH',\n",
    "        'originalW',\n",
    "        'h',\n",
    "        'w'\n",
    "    ]\n",
    "\n",
    "    for font_path in font_paths:\n",
    "        print(f\"Splitting {font_path} into Train-Val-Test datasets...\")\n",
    "        \n",
    "        dataset = pd.read_csv(font_path)\n",
    "        dataset = dataset.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "        train_temp, test_split = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "        train_split, val_split = train_test_split(train_temp, test_size=0.25)\n",
    "\n",
    "        train_dataset = pd.concat([train_dataset, train_split], ignore_index=True)\n",
    "        val_dataset = pd.concat([val_dataset, val_split], ignore_index=True)\n",
    "        test_dataset = pd.concat([test_dataset, test_split], ignore_index=True)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb54ac-bffe-45df-8f4e-53634eda664e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, val, test = create_train_val_test_datasets(font_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2aaed-db26-417f-9b4e-b2f54562e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f8862-94ff-4eec-aa62-ec892ee06a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5556f6f-92d3-479a-8c38-c09c85f08a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f314435b-b8f8-45a0-ba33-5e0f4fa4bf4c",
   "metadata": {},
   "source": [
    "##### Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9960978-cfca-4284-81a1-c8f366409d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"./large-dataset/train.csv\", index=False)\n",
    "val.to_csv(\"./large-dataset/val.csv\", index=False)\n",
    "test.to_csv(\"./large-dataset/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d1292-3d73-4124-bd2c-fd4e3361b0ad",
   "metadata": {},
   "source": [
    "### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb939bf-e9c1-4c57-923b-d983bc82133a",
   "metadata": {},
   "source": [
    "##### Import dataset and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f8dc4-261d-42b5-917f-640f1737d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def preprocess_data(train_path, val_path, test_path):\n",
    "    train_dataset = pd.read_csv(train_path)\n",
    "    val_dataset = pd.read_csv(val_path)\n",
    "    test_dataset = pd.read_csv(test_path)\n",
    "\n",
    "    X_train = train_dataset.drop('font', axis=1).astype('float32').values\n",
    "    y_train = train_dataset['font'].values\n",
    "\n",
    "    X_val = train_dataset.drop('font', axis=1).astype('float32').values\n",
    "    y_val = train_dataset['font'].values\n",
    "\n",
    "    X_test = train_dataset.drop('font', axis=1).astype('float32').values\n",
    "    y_test = train_dataset['font'].values\n",
    "\n",
    "    unique_labels = np.unique(y_train)\n",
    "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    y_train_encoded = np.array([label_to_index[label] for label in y_train])\n",
    "    y_val_encoded = np.array([label_to_index[label] for label in y_val])\n",
    "    y_test_encoded = np.array([label_to_index[label] for label in y_test])\n",
    "\n",
    "    num_classes = len(unique_labels)\n",
    "\n",
    "    y_train = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "    y_val = to_categorical(y_val_encoded, num_classes=num_classes)\n",
    "    y_test = to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, num_classes, unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc413f6-a3a0-475b-b58d-4ca13b735062",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"https://github.com/PavelKilko/applied-application-of-neural-networks/raw/refs/heads/master/lab-6-7/large-dataset/train.csv\"\n",
    "val_path = \"https://github.com/PavelKilko/applied-application-of-neural-networks/raw/refs/heads/master/lab-6-7/large-dataset/val.csv\"\n",
    "test_path = \"https://github.com/PavelKilko/applied-application-of-neural-networks/raw/refs/heads/master/lab-6-7/large-dataset/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd2f11-4fb5-4c0b-b30b-daaefb5982b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, num_classes, unique_labels = preprocess_data(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e60f77-d79c-450c-8413-8c5c81305dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13db51d-a9bd-47d5-8a98-a7a0156575ca",
   "metadata": {},
   "source": [
    "##### Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec72c1b-ca47-4906-a6ff-90fa356b17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3e340-05f9-40ca-b188-695204f6da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model_v0_4(neuron_count, X_train, y_train, X_val, y_val, X_test, y_test, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1024, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=40,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=128,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    return model, history, test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84adec-7865-412b-81e5-f6f38f58d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "print(f\"Training 1024-512-256 model...\")\n",
    "\n",
    "model_v0_4, history, test_loss, test_acc = build_and_train_model_v0_3(X_train, y_train, X_val, y_val, X_test, y_test, num_classes)\n",
    "\n",
    "results.append({\"Test Accuracy\": test_acc, \"Test Loss\": test_loss})\n",
    "\n",
    "plt.plot(history.history['accuracy'], label=f'Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label=f'Val Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf26d4-ec59-4f3d-aaa9-1eb7fdef1ced",
   "metadata": {},
   "source": [
    "##### Build confusion matrix for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0cc9e6-a398-4dd0-91c4-5b47031fde42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_confusion_matrix(model, X_test, y_test, class_names=None, figsize=(20, 20)):\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap='Blues', values_format='.0f', ax=plt.gca())\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba4d55-055f-45ad-b2a4-dc453f097ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(model=model_v0_3, X_test=X_test, y_test=y_test, class_names=unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164eadbe-bf55-4ff6-979f-ead8b20c074e",
   "metadata": {},
   "source": [
    "##### Export final model in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb1dac-007e-472b-b839-ca38accf2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v0_3.save(\"./models/large-font-recognizer-v0.4.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
